{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Aggregate-Data\" data-toc-modified-id=\"Aggregate-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Aggregate Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#GroupBy\" data-toc-modified-id=\"GroupBy-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>GroupBy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Index\" data-toc-modified-id=\"Index-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>Index</a></span></li></ul></li></ul></li><li><span><a href=\"#PivotTables\" data-toc-modified-id=\"PivotTables-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>PivotTables</a></span></li><li><span><a href=\"#pd.concat\" data-toc-modified-id=\"pd.concat-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>pd.concat</a></span></li><li><span><a href=\"#pd.merge\" data-toc-modified-id=\"pd.merge-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>pd.merge</a></span><ul class=\"toc-item\"><li><span><a href=\"#Suffixes\" data-toc-modified-id=\"Suffixes-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Suffixes</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "So far, we've learned how to use the pandas library and how to create visualizations with data sets that didn't require much cleanup. However, most data sets in real life require extensive cleaning and manipulation to extract any meaningful insights. In fact, Forbes estimates that data scientists spend about 60% of their time cleaning and organizing data, so it's critical to be able to manipulate data quickly and efficiently.\n",
    "\n",
    "In this course, we'll learn the following:\n",
    "\n",
    "- Data aggregation\n",
    "- How to combine data\n",
    "- How to transform data\n",
    "- How to clean strings with pandas\n",
    "- How to handle missing and duplicate data\n",
    "\n",
    "You'll need some basic knowledge of pandas and matplotlib to complete this course, including:\n",
    "\n",
    "- Basic knowledge of pandas dataframes and series\n",
    "- How to select values and filter a dataframe\n",
    "- Knowledge of data exploration methods in pandas, such as the info and head methods\n",
    "- How to create visualizations in pandas and matplotlib\n",
    "\n",
    "Throughout this course, we'll work to answer the following questions:\n",
    "\n",
    "- How can aggregating the data give us more insight into happiness scores?\n",
    "- How did world happiness change from 2015 to 2017?\n",
    "- Which factors contribute the most to the happiness score?\n",
    "\n",
    "In this mission, we'll start by learning how to aggregate data. Then in the following missions, we'll learn different data cleaning skills that can help us aggregate and analyze the data in different ways. We'll start by learning each topic in isolation, but build towards a more complete data cleaning workflow by the end of the course.\n",
    "\n",
    "## Aggregate Data\n",
    "In this mission, we'll learn how to perform different kinds of **aggregations**, applying a statistical operation to groups of our data, and create visualizations like the one above.\n",
    "\n",
    "Recall that in the Pandas Fundamentals course, we learned a way to use loops for aggregation. Our process looked like this:\n",
    "\n",
    "- Identify each unique group in the data set.\n",
    "- For each group:\n",
    "    - Select only the rows corresponding to that group.\n",
    "    - Calculate the average for those rows.\n",
    "Let's use the same process to find the mean happiness score for each region.\n",
    "\n",
    "### GroupBy\n",
    "Let's break down the code we wrote in the previous screen into three steps:\n",
    "\n",
    "1. Split the dataframe into groups.\n",
    "2. Apply a function to each group.\n",
    "3. Combine the results into one data structure.\n",
    "\n",
    "As with many other common tasks, pandas has a built-in operation for this process. The `groupby` operation performs the \"split-apply-combine\" process on a dataframe, but condenses it into two steps:\n",
    "\n",
    "1. Create a GroupBy object.\n",
    "2. Call a function on the GroupBy object.\n",
    "\n",
    "The GroupBy object, distinct from a dataframe or series object, allows us to split the dataframe into groups, but only in an abstract sense. Nothing is actually computed until a function is called on the GroupBy object.\n",
    "\n",
    "You can think of the `groupby` operation like this. Imagine a dataframe as a structure made of stacking blocks in all different colors and sizes.\n",
    "\n",
    "You know you'll eventually want to group the blocks according to color instead, but you don't know yet what you want to do with them after. Using the groupby process, we would first create a mapping document, the `GroupBy` object, containing information on how to group the blocks by color and where each block is located in the original structure.\n",
    "\n",
    "Once we create the mapping document, we can use it to easily rearrange the blocks into different structures. For example, let's say our manager asks us first to build another structure using the biggest block from each color.\n",
    "\n",
    "Creating the initial mapping document, or GroupBy object, allows us to optimize our work, because we no longer have to refer back to the original dataframe. By working with the `groupby` operation, we make our code faster, more flexible, and easier to read.\n",
    "\n",
    "The first step in the groupby operation is to create a GroupBy object:\n",
    "\n",
    "To create a GroupBy object, we use the `DataFrame.groupby()` method:\n",
    "\n",
    "`df.groupby('col')`\n",
    "\n",
    "where `col` is the column you want to use to group the data set. Note that you can also group the data set on multiple columns by passing a list into the `DataFrame.groupby()` method. However, for teaching purposes, we'll focus on grouping the data by just one column in this mission.\n",
    "\n",
    "When choosing the column, think about which columns could be used to split the data set into groups. To put it another way, look at columns with the same value for multiple rows.\n",
    "\n",
    "We can see from the couple of rows above that the `Region` column fits this criteria. Let's confirm the number of regions and the number of unique values in each region for the entire dataframe with the `Series.value_counts()` method next:\n",
    "\n",
    "`happiness2015['Region'].value_counts()`\n",
    "\n",
    "Since there's a small number of groups and each group contains more than one unique value, we can confirm the Region column is a good candidate to group by.\n",
    "\n",
    "Next, let's create a Groupby object and group the dataframe by the `Region` column:\n",
    "\n",
    "`happiness2015.groupby('Region')`\n",
    "\n",
    "`print(happiness2015.groupby('Region'))`\n",
    "\n",
    "`< pandas.core.groupby.groupby.DataFrameGroupBy object at 0x7f77882fa470 >`\n",
    "\n",
    "Don't be alarmed! This isn't an error. This is telling us that an object of type GroupBy was returned, just like we expected.\n",
    "\n",
    "Before our we start aggregating data, we'll build some intuition around GroupBy objects. We'll start by using the `GroupBy.get_group()` method to select data for a certain group.\n",
    "\n",
    "As an example, to select the data for just the North America group, we'd pass 'North America' into the `get_group()` method as follows:\n",
    "\n",
    "In the last exercise, we used the `GroupBy.get_group()` method to select the `Australia and New Zealand` group. The result is a dataframe containing just the rows for the countries in the `Australia and New Zealand` group:\n",
    "\n",
    "We can also use the `GroupBy.groups` [attribute](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.groups.html) to get more information about the GroupBy object:\n",
    "\n",
    "`grouped = happiness2015.groupby('Region')\n",
    "grouped.groups`\n",
    "\n",
    "The result is a dictionary in which each key corresponds to a region name. See below for the first couple of keys:\n",
    "\n",
    "Notice that the values include the index for each row in the original happiness2015 dataframe with the corresponding region name. To prove this, let's again look at the data for the Australia and New Zealand group:\n",
    "\n",
    "And we see that those rows correspond to Australia and New Zealand! Notice that the `get_group()` method also returned the same dataframe above.\n",
    "\n",
    "Next, let's continue building our intuition by practicing using the `groups` attribute and `get_group()` method.\n",
    "\n",
    "In the last exercise, we confirmed that the values for the 'North America' group returned by `grouped.groups` do correspond to the countries in North_America in the `happiness2015` dataframe.\n",
    "\n",
    "Now that we have a good understanding of `GroupBy` objects, let's use them to **aggregate** our data. In order to aggregate our data, we must call a function on the GroupBy object.\n",
    "\n",
    "**SIZE**\n",
    "A basic example of aggregation is computing the number of rows for each of the groups. We can use the `GroupBy.size()` method to confirm the size of each region group:\n",
    "\n",
    "`grouped = happiness2015.groupby('Region')\n",
    "grouped.groups`\n",
    "\n",
    "Notice that the result is a Series and contains just one value for each group. Each value represents the number of rows in each group. For example, the 'Australia and New Zealand' group contains two rows.\n",
    "\n",
    "Pandas has built in a number of other [common aggregation methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html):\n",
    "\n",
    "Let's practicing use one of these aggregation methods next.\n",
    "\n",
    "#### Index\n",
    "You may have noticed that Region appears in a different row than the rest of the column names. Because we grouped the dataframe by region, the unique values in Region are used as the index. Up until now, we've mostly worked with dataframes with a numeric index.\n",
    "\n",
    "However, what if we wanted to apply more than one kind of aggregation to a column at a time?\n",
    "\n",
    "For example, suppose we wanted to calculate both the mean and maximum happiness score for each region. Using what we learned so far, we'd have to first calculate the mean, like we did above, and then calculate the maximum separately.\n",
    "\n",
    "Luckily, however, the `GroupBy.agg()` [method](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.DataFrameGroupBy.agg.html) can perform both aggregations at once. We can use the following syntax:\n",
    "\n",
    "`GroupBy.agg([func_1, func_2, func_3])`\n",
    "\n",
    "Note that when we pass the functions into the `agg()` method as arguments, we don't use parentheses after the function names. For example, when we use `np.mean`, we refer to the function object itself and treat it like a variable, whereas `np.mean()` would be used to call the function and get the returned value.\n",
    "\n",
    "The function names can also be passed in as strings, but we won't cover that explicitly in this mission. You can refer to [this documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) for more information on this topic.\n",
    "\n",
    "Let's practice using the `agg()` method next.\n",
    "\n",
    "In the last exercise, we learned we can use the GroupBy.agg() method to:\n",
    "\n",
    "Perform more than one aggregation at once.\n",
    "\n",
    "1. Compute custom aggregations.\n",
    "2. To compute multiple aggregations at once, we passed a list of the function names into the agg method:\n",
    "\n",
    "Our result is a dataframe containing both the mean and maximum happiness scores for each region. Note that the columns are named for the functions themselves. Because `np.max` is an alias for `np.amax`, the column for the maximum is named `amax`.\n",
    "\n",
    "Above, we calculated the difference between the mean and maximum values for each region. Because the `agg()` method allows us to create custom aggregation functions, it gives us much more flexibility in how we can transform our data.\n",
    "\n",
    "`def dif(group):\n",
    "    return(group.max() - group.mean())\n",
    "happy_grouped.agg(dif)`\n",
    "\n",
    "Up until this point, we've deliberately split creating the GroupBy object and calling a function into separate steps to make it easier to understand. However, if you read through other teaching resources, you may see instances in which the statements are combined:\n",
    "\n",
    "Both approaches will return the same result. However, if you plan on computing multiple aggregations with the same GroupBy object, it's recommended to save the object to a variable first. You may wish to save it to a variable in all cases to make your code easier to understand. As we compute more complex aggregations, the syntax can get confusing!\n",
    "\n",
    "## PivotTables\n",
    "`Index` and `values` are actually arguments used in another method used to aggregate data - the `DataFrame.pivot_table()` [method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html). This `df.pivot_table()` method can perform the same kinds of aggregations as the `df.groupby` method and make the code for complex aggregations easier to read.\n",
    "\n",
    "If you're an Excel user, you may have already drawn comparisons between the groupby operation and Excel pivot tables. If you've never used Excel, don't worry! No prior knowledge is needed for this mission. We'll demonstrate the `pivot_table()` method next.\n",
    "\n",
    "Below, we use the `df.pivot_table()` method to perform the same aggregation as above.\n",
    "\n",
    "`happiness2015.pivot_table(values='Happiness Score', index='Region', aggfunc=np.mean)`\n",
    "\n",
    "Keep in mind that this method returns a dataframe, so normal dataframe filtering and methods can be applied to the result. For example, let's use the `DataFrame.plot()` method to create a visualization. Note that we exclude `aggfunc` below because the mean is the default aggregation function of `df.pivot_table()`.\n",
    "\n",
    "`pv_happiness = happiness2015.pivot_table('Happiness Score', 'Region')\n",
    "pv_happiness.plot(kind='barh', title='Mean Happiness Scores by Region', xlim=(0,10), legend=False)`\n",
    "\n",
    "Next, let's explore a feature unique to the `df.pivot_table()` method.\n",
    "\n",
    "In the last exercise, we learned that when we set the `margins` parameter equal to `True`, `All` will be added to the index of the resulting dataframe and the corresponding value will be the result of applying the aggregation method to the entire column. In our example, `All` is the mean of the `Happiness Score` column.\n",
    "\n",
    "The `pivot_table` method also allows us to aggregate multiple columns and apply multiple functions at once.\n",
    "\n",
    "Below, we aggregate both the 'Happiness Score' and 'Family' columns in `happiness2015` and group by the 'Region' column:\n",
    "\n",
    "`happiness2015.pivot_table(['Happiness Score', 'Family'], 'Region')`\n",
    "\n",
    "To apply multiple functions, we can pass a list of the functions into the aggfunc parameter:\n",
    "\n",
    "`happiness2015.pivot_table('Happiness Score', 'Region', aggfunc=[np.mean, np.min , np.max], margins=True)`\n",
    "\n",
    "Let's compare the results returned by the `groupby` operation and the `pivot_table` method next.\n",
    "\n",
    "In this mission, we learned how to use the `groupby` operation and the `df.pivot_table()` method for aggregation. In the next mission, we'll learn how to combine data sets using the `pd.concat()` and `pd.merge()` functions.\n",
    "\n",
    "## pd.concat\n",
    "In the last mission, we worked with just one data set, the 2015 World Happiness Report, to explore data aggregation. However, it's very common in practice to work with more than one data set at a time.\n",
    "\n",
    "Often, you'll find that you need additional data to perform analysis or you'll find that you have the data, but need to pull it from mulitiple sources. In this mission, we'll learn a couple of different techniques for combining data using pandas to easily handle situations like these.\n",
    "\n",
    "Below are descriptions for some of the columns:\n",
    "\n",
    "- `Country` - Name of the country\n",
    "- `Region` - Name of the region the country belongs to\n",
    "- `Happiness Rank` - The rank of the country, as determined by its happiness score\n",
    "- `Happiness Score` - A score assigned to each country based on the answers to a poll question that asks respondents to rate their happiness on a scale of 0-10\n",
    "\n",
    "Let's start by reading the 2015, 2016, and 2017 reports into a pandas dataframe and adding a `Year` column to each to make it easier to distinguish between them.\n",
    "\n",
    "Let's start by exploring the `pd.concat()` [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html). The `concat()` function combines dataframes one of two ways:\n",
    "\n",
    "1. Stacked: Axis = 0 (This is the default option.)\n",
    "2. Side by Side: Axis = 1\n",
    "\n",
    "**Since `concat` is a function, not a method, we use the syntax below:**\n",
    "\n",
    "`pd.concat([df1, df2])`\n",
    "\n",
    "In the next exercise, we'll use the `concat()` function to combine subsets of `happiness2015` and `happiness2016` and then debrief the results on the following screen.\n",
    "\n",
    "When you reviewed the results from the last exercise, you probably noticed that we merely pushed the dataframes together vertically or horizontally - none of the values, column names, or indexes changed. For this reason, when you use the `concat()` function to combine dataframes with the same shape and index, you can think of the function as \"gluing\" dataframes together.\n",
    "\n",
    "However, what happens if the dataframes have different shapes or columns? Let's confirm the `concat()` function's behavior when we combine dataframes that don't have the same shape in the next exercise.\n",
    "\n",
    "In the last exercise, we saw that the analogy of \"gluing\" dataframes together doesn't fully describe what happens when concatenating dataframes of different shapes. Instead, the function combined the data according to the corresponding column names:\n",
    "\n",
    "**Note** that because the `Standard Error` column didn't exist in `head_2016`, `NaN` values were created to signify those values are missing. By default, the `concat` function will keep ALL of the data, no matter if missing values are created.\n",
    "\n",
    "Also, notice again the indexes of the original dataframes didn't change. If the indexes aren't meaningful, it can be better to reset them. This is especially true when we create duplicate indexes, because they could cause errors as we perform other data cleaning tasks.\n",
    "\n",
    "Luckily, the `concat` function has a parameter, `ignore_index`, that can be used to clear the existing index and reset it in the result. Let's practice using it next.\n",
    "\n",
    "`concat_update_index = pd.concat([head_2015, head_2016], ignore_index=True)`\n",
    "\n",
    "## pd.merge\n",
    "Next, we'll explore the `pd.merge()` [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) - a function that can execute high performance database-style joins. Note that unlike the `concat` function, the `merge` function only combines dataframes horizontally (axis=1) and can only combine two dataframes at a time. However, it can be valuable when we need to combine very large dataframes quickly and provides more flexibility in terms of how data can be combined, as we'll see in the next couple screens.\n",
    "\n",
    "With the `merge()` function, we'll combine dataframes on a **key**, a shared index or column. When choosing a key, it's good practice to use keys with unique values to avoid duplicating data.\n",
    "\n",
    "You can think of keys as creating a link from one dataframe to another using the common values or indexes. For example, in the diagram below, we linked the dataframes using common values in the `Country` columns.\n",
    "\n",
    "`pd.merge(left=df1, right=df2, on=col_name`\n",
    "\n",
    "Since we joined the dataframes on the `Country` column, or used it as the key, the `merge()` function looked to match elements in the Country column in BOTH dataframes.\n",
    "\n",
    "This way of combining, or joining, data is called an inner join. An inner join returns only the intersection of the keys, or the elements that appear in both dataframes with a common key.\n",
    "\n",
    "The term \"join\" originates from SQL (or structured query language), a language used to work with databases. If you're a SQL user, you'll recognize the following concepts. If you've never used SQL, don't worry! No prior knowledge is neccessary for this mission, but we will learn SQL later in this path.\n",
    "\n",
    "There are actually four different types of joins:\n",
    "\n",
    "- `Inner`: only includes elements that appear in both dataframes with a common key\n",
    "- `Outer`: includes all data from both dataframes\n",
    "- `Left`: includes all of the rows from the \"left\" dataframe along with any rows from the \"right\" dataframe with a common key; the result retains all columns from both of the original dataframes\n",
    "- `Right`: includes all of the rows from the \"right\" dataframe along with any rows from the \"left\" dataframe with a common key; the result retains all columns from both of the original dataframes\n",
    "\n",
    "If the definition for outer joins sounds familiar, it's because we've already seen examples of outer joins! Recall that when we combined data using the `concat` function, it kept all of the data from all dataframes, no matter if missing values were created.\n",
    "\n",
    "Since it's much more common to use inner and left joins for database-style joins, we'll focus on these join types for the remainder of the mission, but encourage you to explore the other options on your own.\n",
    "\n",
    "Let's experiment with changing the join type next. The `how` parameter defines what type of join you'll use for the `merge`:\n",
    "\n",
    "`pd.merge(left=three_2015, right=three_2016, on='Country', how='left')`\n",
    "\n",
    "### Suffixes\n",
    "Let's summarize what we learned in the last exercise:\n",
    "\n",
    "1. Changing the join type from an inner join to a left join resulted in a dataframe with more rows and created NaNs.\n",
    "2. When using a left join, interchanging the dataframes assigned to the left and right parameters changes the results.\n",
    "\n",
    "Let's look into the results in more detail. First, let's look at the case in which the \"left\" dataframe is three_2015 and the \"right\" dataframe is three_2016:\n",
    "\n",
    "In summary, we'd use a left join when we don't want to drop any data from the left dataframe.\n",
    "\n",
    "Note that a right join works the same as a left join, except it includes all of the rows from the \"right\" dataframe. Since it's far more common in practice to use a left join, we won't cover right joins in detail.\n",
    "\n",
    "You may have also noticed above that the `merge` function added a suffix of either `_x` or `_y` to columns of the same name to distinguish between them. The `suffixes` parameter allows you to change the column headers of the items you're joining.\n",
    "\n",
    "`pd.merge(left=three_2016, right=three_2015, how='left', on='Country', suffixes=('_2016', '_2015'))`\n",
    "\n",
    "### Joining DFs on the Index"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
